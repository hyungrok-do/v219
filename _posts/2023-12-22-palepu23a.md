---
abstract: In this paper, we introduce a novel regularization scheme on contrastive
  language-image pre-trained (CLIP) medical vision models. Our approach is based on
  the observation that, for many medical imaging tasks, text tokens should only describe
  a small number of image regions and, likewise, each image region should correspond
  to only a few text tokens. In CLIP-style models, this implies that text-token embeddings
  should have high similarity to only a small number of image-patch embeddings for
  a given image-text pair. We formalize this observation using a novel regularization
  scheme that penalizes the entropy of the text-token to image-patch similarity scores.
  We qualitatively and quantitatively demonstrate that the proposed regularization
  scheme improves localization by shrinking most of the pairwise text-token and image-patch
  similarity scores towards zero, thus achieving the desired effect. We demonstrate
  the promise of our approach in an important medical context, chest x-rays, where
  this underlying sparsity hypothesis naturally arises. Using our proposed approach,
  we achieve state of the art (SOTA) average zero-shot performance on the CheXpert
  and Padchest chest x-ray datasets, outperforming an unregularized version of the
  model and several recently published self-supervised models.
booktitle: Proceedings of the 8th Machine Learning for Healthcare Conference
title: 'TIER: Text-Image Entropy Regularization for Medical CLIP-style models'
volume: '219'
year: '2023'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: palepu23a
month: 0
tex_title: 'TIER: Text-Image Entropy Regularization for Medical CLIP-style models'
firstpage: 548
lastpage: 564
page: 548-564
order: 548
cycles: false
bibtex_author: Palepu, Anil and Beam, Andrew
author:
- given: Anil
  family: Palepu
- given: Andrew
  family: Beam
date: 2023-12-22
address:
container-title: Proceedings of the 8th Machine Learning for Healthcare Conference
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 22
pdf: https://proceedings.mlr.press/v219/palepu23a/palepu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
